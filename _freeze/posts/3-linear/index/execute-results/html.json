{
  "hash": "6d03af4d55782ce0b843f0883300e47b",
  "result": {
    "markdown": "---\ntitle: \"Linear & Non-linear Regression\"\nauthor: \"Sarah Ramboyong\"\ndate: \"2023-12-6\"\nimage: \"linear_image.png\"\n---\n\n# Introduction\n\nRegression is a statitical method that allows on to predict the output of one variables based on the input of another. It is a way of quantifying the relationship between a dependent variable (Y) and one or more independent variables (X).\n\nThere are two types of regression: linear and non-linear. Linear regression uses one independent variable to predict the outcome of the dependent variable. It follows the equation: $y=mx+b$ where m is the slope, or the rate of change, and b is the intercept.\n\n\nNon-linear regression is used when there is a relationship between the dependent variable and more than one indepent variables. There are multiple types of non-linear regression, such as\n* polynomial regression\n* exponential regression\n* logarithmic regression\n\nNon-linear regression accounts for the majority of real-life data, which are complex and often influenced by multiple variables.\n\nIn this blog post we will use matplotlib and sklearn to plot different types of regression. We will also explore the ability to perform prediction using regression. We will be using the Boston Housing dataset from the StatLib archive. This dataset contains 506 samples and has a known linear relationship between variables. It has 14 different features:\n* Per capita crime rate by town\n* Proportion of residential land zoned for lots over 25,000 sq.ft.\n* Proportion of non-retail business acres per town.\n* Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n* Nitric oxides concentration (parts per 10 million)\n* Average number of rooms per dwelling\n* Proportion of owner-occupied units built prior to 1940\n* Weighted distances to five Boston employment centres\n* Index of accessibility to radial highways\n* Full-value property-tax rate per $10,000\n* Pupil-teacher ratio by town\n* Proportion of blacks by town\n* % lower status of the population\n* Median value of owner-occupied homes in $1000's\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\n\n# Load the Boston housing dataset\nboston = pd.read_csv(\"C:/Users/sarah/git/CS5805-Blog/posts/3-linear/BostonHousing.csv\")\nboston.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>crim</th>\n      <th>zn</th>\n      <th>indus</th>\n      <th>chas</th>\n      <th>nox</th>\n      <th>rm</th>\n      <th>age</th>\n      <th>dis</th>\n      <th>rad</th>\n      <th>tax</th>\n      <th>ptratio</th>\n      <th>b</th>\n      <th>lstat</th>\n      <th>medv</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00632</td>\n      <td>18.0</td>\n      <td>2.31</td>\n      <td>0</td>\n      <td>0.538</td>\n      <td>6.575</td>\n      <td>65.2</td>\n      <td>4.0900</td>\n      <td>1</td>\n      <td>296</td>\n      <td>15.3</td>\n      <td>396.90</td>\n      <td>4.98</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.02731</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0</td>\n      <td>0.469</td>\n      <td>6.421</td>\n      <td>78.9</td>\n      <td>4.9671</td>\n      <td>2</td>\n      <td>242</td>\n      <td>17.8</td>\n      <td>396.90</td>\n      <td>9.14</td>\n      <td>21.6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.02729</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0</td>\n      <td>0.469</td>\n      <td>7.185</td>\n      <td>61.1</td>\n      <td>4.9671</td>\n      <td>2</td>\n      <td>242</td>\n      <td>17.8</td>\n      <td>392.83</td>\n      <td>4.03</td>\n      <td>34.7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.03237</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0</td>\n      <td>0.458</td>\n      <td>6.998</td>\n      <td>45.8</td>\n      <td>6.0622</td>\n      <td>3</td>\n      <td>222</td>\n      <td>18.7</td>\n      <td>394.63</td>\n      <td>2.94</td>\n      <td>33.4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.06905</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0</td>\n      <td>0.458</td>\n      <td>7.147</td>\n      <td>54.2</td>\n      <td>6.0622</td>\n      <td>3</td>\n      <td>222</td>\n      <td>18.7</td>\n      <td>396.90</td>\n      <td>5.33</td>\n      <td>36.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n# Linear Regression\n\nFirst we need to identify which variables have a relationship with the target feature, median value of owner-occupied homes in $1000's. Todo this we can calcualte the correlation between each variable and the target feature. Correlation numbers will be between -1 and 1, with a positive correlation indicating a positive relationship, negative indicating a negative relationship, and 0 indiciating there is no relationship between variables.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Calculate the correlation of each variable with the target variable\ncorr_with_target = boston.corr()['medv'].sort_values(ascending=False)\n\nprint(corr_with_target)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nmedv       1.000000\nrm         0.695360\nzn         0.360445\nb          0.333461\ndis        0.249929\nchas       0.175260\nage       -0.376955\nrad       -0.381626\ncrim      -0.388305\nnox       -0.427321\ntax       -0.468536\nindus     -0.483725\nptratio   -0.507787\nlstat     -0.737663\nName: medv, dtype: float64\n```\n:::\n:::\n\n\nBy calculating the correlation between each variable we see that the average number of rooms per dwelling has the highest correlation coefficient with the median value of owner-occupied homes. The correlation value of 0.695 shows a positive correlation between variables.\n\nNow we will plot these variables to better visualize and understand their relationship. We will also use numpy's built-in method to fit the linear regression line.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\n# Create a LinearRegression object\nlr = LinearRegression()\n\n# Reshape the data to fit the model\nX = boston[\"rm\"].values.reshape(-1,1)\ny = boston[\"medv\"].values\n\n# Fit the model to the data\nlr.fit(X, y)\n\n# Get the coefficients and print\nm = lr.coef_\nb = lr.intercept_ \nprint(f'Coefficient: {lr.coef_}')\nprint(f'Intercept: {lr.intercept_}')\n\n# Plot the relationship between RM and MEDV\nplt.scatter(X, y)\nplt.title('Relationship between RM and MEDV')\nplt.xlabel('Average number of rooms per dwelling (RM)')\nplt.ylabel('Median value of owner-occupied homes (MEDV)')\nplt.plot(boston['rm'], m*boston['rm'] + b, color='red')\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCoefficient: [9.10210898]\nIntercept: -34.670620776438554\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-2.png){width=585 height=449}\n:::\n:::\n\n\nWe can see that the regression line goes down the center of the data and appears to be a good fit. We also see that the coefficient is 9.1 and the intercept is -34.67. This means that as the average number of rooms per dwelling increase, the median value of owner-occupied homes increases by 9.1. \n \n# Predicting with Linear Regression\n\n We can also perform predictions using linear regresion. First we will split the dataset so that we can perform predictions on the dataset. We will fit a regression line like in the previous step but only using the training set, and then we will input the testing set into our model and predict the median value of owner-occurpied homes. We will then compare that estimate to the actual value.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Split the dataset into a training set and a testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a LinearRegression object\nlr = LinearRegression()\n\n# Fit the model using the training set\nlr.fit(X_train, y_train)\n\n# Get the coefficients and print\nm = lr.coef_\nb = lr.intercept_ \nprint(f'Coefficient: {lr.coef_}')\nprint(f'Intercept: {lr.intercept_}')\n\n# Make predictions using the testing set\ny_pred = lr.predict(X_test)\n\n# Plot the relationship between RM and MEDV\nplt.scatter(X_test, y_test)\nplt.title('Relationship between RM and MEDV')\nplt.xlabel('Average number of rooms per dwelling (RM)')\nplt.ylabel('Median value of owner-occupied homes (MEDV)')\nplt.plot(X_test, y_pred, color='red')\n\nplt.show()\n\n# Calculate the metrics\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f'MSE: {mse}')\nprint(f'R-squared: {r2}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCoefficient: [9.34830141]\nIntercept: -36.24631889813792\nMSE: 46.144775347317264\nR-squared: 0.3707569232254778\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-2.png){width=585 height=449}\n:::\n:::\n\n\nThe regression line still appears to be a good fit. This model has a coefficient of 9.35 and an intercept of -36.25, both of which are close to the values of the first model.\n\nWe can evalute this quantitativly using Mean Square Error (MSE) and R-squared values. We want as little error as possible and from this model we achieved a MSE of 46.14, which is relatively good. R-squared is a measure of how well the dependent variable is explained by the independent variable. generally a higher R-squared indicates a better fit. We achieved a R-squared of 0.37 which is relatively good. These metrics indicate that additional variables have an effect on the median value of owner-occupied homes. \n\n# Non-Linear Regression\n\nWe will attempt to approve the regression model above by instead fitting the data for a non-linear regression. We will repeat the process above but for a 2nd degree polynomial.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Split the dataset into a training set and a testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a PolynomialFeatures object for 2nd degree polynomial\npoly = PolynomialFeatures(degree=2)\n\n# Transform the x data for a quadratic fit: \nX_poly = poly.fit_transform(X_train)\n\n# Fit the transformed features to Linear Regression\npoly_reg = LinearRegression()\npoly_reg.fit(X_poly, y_train)\n\n# Transform the test data into polynomial features\nX_test_poly = poly.transform(X_test)\n\n# Predicting on the test data\ny_pred = poly_reg.predict(X_test_poly)\n\nprint(f'Coefficients: {poly_reg.coef_}')\nprint(f'Intercept: {poly_reg.intercept_}')\n\n# Plot the original data\nplt.scatter(X_train, y_train, color = 'blue')\n\n# Plot the polynomial regression line\nX_line = np.linspace(min(X_train), max(X_train), 100).reshape(-1, 1)\nX_line_poly = poly.transform(X_line)\ny_line = poly_reg.predict(X_line_poly)\nplt.plot(X_line, y_line, color = 'red')\n\nplt.title('Non-linear Regression')\nplt.xlabel('Average number of rooms per dwelling (RM)')\nplt.ylabel('Median value of owner-occupied homes (MEDV)')\nplt.show()\n\n# Calculate the metrics\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f'MSE: {mse}')\nprint(f'R-squared: {r2}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCoefficients: [  0.         -19.19134842   2.19434514]\nIntercept: 55.37179190740247\nMSE: 35.36977373731789\nR-squared: 0.5176878620868068\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-2.png){width=586 height=449}\n:::\n:::\n\n\nWe see that now we have 2 coefficients, -19.19 and 2.19. The intercept also changed drastically, increasing to 55.37. We can also see visually that the line better fits the data, especially on the ends. Running our evaluation metrics again we see that our MSE decreased to 35.37, and our R-squared value increased to 0.52.\n\n# Performance Improvement\n\nFinally, we will find the best dress for the polynomial regression model by looping through models with degrees 1 to 8.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Define the maximum degree\nmax_degree = 8\n\n# Initialize the minimum MSE and the best degree\nmin_mse = None\nbest_degree = None\nbest_r2 = None\n\n# Split the dataset into a training set and a testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Loop over degrees from 1 to max_degree\nfor degree in range(1, max_degree+1):\n    # Create a PolynomialFeatures object for the current degree\n    poly = PolynomialFeatures(degree=degree)\n\n    # Transform the x data for a polynomial fit\n    X_poly = poly.fit_transform(X_train)\n\n    # Fit the transformed features to Linear Regression\n    poly_reg = LinearRegression()\n    poly_reg.fit(X_poly, y_train)\n\n    # Transform the test data into polynomial features\n    X_test_poly = poly.transform(X_test)\n\n    # Predict on the test data\n    y_pred = poly_reg.predict(X_test_poly)\n\n    # Calculate the MSE\n    mse = mean_squared_error(y_test, y_pred)\n\n    # Calcualte R-squared\n    r2 = r2_score(y_test, y_pred)\n\n    print(f'Degree: {degree}')\n    print(f'MSE: {mse}')\n    print(f'R2: {r2}\\n')\n\n\n    # If the current MSE is smaller than the minimum MSE, update the minimum MSE and the best degree\n    if min_mse is None or mse < min_mse:\n        min_mse = mse\n        best_degree = degree\n        best_r2 = r2\n\nprint(f'Best degree: {best_degree}')\nprint(f'Minimum MSE: {min_mse}')\nprint(f'Best R2: {best_r2}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDegree: 1\nMSE: 46.144775347317264\nR2: 0.3707569232254778\n\nDegree: 2\nMSE: 35.36977373731789\nR2: 0.5176878620868068\n\nDegree: 3\nMSE: 34.44520666491044\nR2: 0.5302955175569488\n\nDegree: 4\nMSE: 39.57799328447852\nR2: 0.4603034020766015\n\nDegree: 5\nMSE: 32.51475117174327\nR2: 0.5566197491726392\n\nDegree: 6\nMSE: 35.62776489457643\nR2: 0.5141698224311337\n\nDegree: 7\nMSE: 38.89503704829279\nR2: 0.4696163845350768\n\nDegree: 8\nMSE: 34.69742601915864\nR2: 0.5268561838231798\n\nBest degree: 5\nMinimum MSE: 32.51475117174327\nBest R2: 0.5566197491726392\n```\n:::\n:::\n\n\nRunning this loop we find that the best degree for this dataset is 5, with a MSE of 32.51 and R-squared of 0.56. Beyond degree 5 we see that we begin to over fit the model and MSE begins to increase.\n\nBelow is the plot for the optimal non-linear, polynomial regression model.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nimport numpy as np\n\n# Split the dataset into a training set and a testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a PolynomialFeatures object for the best degree\npoly = PolynomialFeatures(degree=best_degree)\n\n# Transform the x data for a polynomial fit\nX_poly = poly.fit_transform(X_train)\n\n# Fit the transformed features to Linear Regression\npoly_reg = LinearRegression()\npoly_reg.fit(X_poly, y_train)\n\n# Plot the original data\nplt.scatter(X_train, y_train, color = 'blue')\n\n# Plot the polynomial regression line\nX_line = np.linspace(min(X_train), max(X_train), 100).reshape(-1, 1)\nX_line_poly = poly.transform(X_line)\ny_line = poly_reg.predict(X_line_poly)\nplt.plot(X_line, y_line, color = 'red')\n\nplt.title('Best Non-linear Regression Model')\nplt.xlabel('Average number of rooms per dwelling (RM)')\nplt.ylabel('Median value of owner-occupied homes (MEDV)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=586 height=449}\n:::\n:::\n\n\n## Referenes:\n* https://www.investopedia.com/terms/r/regression.asp#:~:text=%25%2025%25%200%25-,What%20Is%20a%20Regression%3F,(known%20as%20independent%20variables).\n* https://www.seldon.io/machine-learning-regression-explained\n* https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html\n* https://lib.stat.cmu.edu/datasets/boston\n* https://pieriantraining.com/nonlinear-regression-in-machine-learning-python/\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}