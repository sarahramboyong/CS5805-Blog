[
  {
    "objectID": "posts/5-anomaly/index.html",
    "href": "posts/5-anomaly/index.html",
    "title": "Anomaly & Outlier Detection",
    "section": "",
    "text": "Anomaly detection is the process of identifying suspicious or unusual occurrences within the dataset. These points are also called outliers and the can effect the performance of machine learning algorithms and skew results. In this blog we will implement one method of anomaly detection, Density-Based Spatial Clustering of Applications with Noise (DBSCAN). We will be using the Iris dataset from sklearn. This dataset contains 150 samples from 3 classes, 50 instances each. It contains the length and width of both sepals and petals.\nThis dataset is publically avaliable through the sklearn package with [documentation avaliable].(https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)\n\nfrom sklearn.datasets import load_iris\n\n# Load the iris dataset\niris = load_iris()\n\n# The data, target, and feature names are stored in the data, target, and feature_names attributes, respectively\nX = iris.data\ny = iris.target\nfeature_names = iris.feature_names"
  },
  {
    "objectID": "posts/5-anomaly/index.html#references",
    "href": "posts/5-anomaly/index.html#references",
    "title": "Anomaly & Outlier Detection",
    "section": "References:",
    "text": "References:\n\nhttps://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\nhttps://pieriantraining.com/dbscan-for-outlier-detection-in-python-and-scikit-learn-machine-learning-in-python/\nhttps://www.geeksforgeeks.org/detect-and-remove-the-outliers-using-python/\nhttps://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html#:~:text=low%20point%20density.-,Density%2DBased%20Spatial%20Clustering%20of%20Applications%20with%20Noise%20(DBSCAN),is%20containing%20noise%20and%20outliers."
  },
  {
    "objectID": "posts/3-linear/index.html",
    "href": "posts/3-linear/index.html",
    "title": "Linear & Non-linear Regression",
    "section": "",
    "text": "Regression is a statitical method that allows on to predict the output of one variables based on the input of another. It is a way of quantifying the relationship between a dependent variable (Y) and one or more independent variables (X).\nThere are two types of regression: linear and non-linear. Linear regression uses one independent variable to predict the outcome of the dependent variable. It follows the equation: \\(y=mx+b\\) where m is the slope, or the rate of change, and b is the intercept.\nNon-linear regression is used when there is a relationship between the dependent variable and more than one indepent variables. It follows the equation:\nNon-linear regression, or multi-linear regression, accounts for the majority of real-life data which are complex and often influences b\n\n\n\nhttps://www.investopedia.com/terms/r/regression.asp#:~:text=%25%2025%25%200%25-,What%20Is%20a%20Regression%3F,(known%20as%20independent%20variables)."
  },
  {
    "objectID": "posts/3-linear/index.html#referenes",
    "href": "posts/3-linear/index.html#referenes",
    "title": "Linear & Non-linear Regression",
    "section": "",
    "text": "https://www.investopedia.com/terms/r/regression.asp#:~:text=%25%2025%25%200%25-,What%20Is%20a%20Regression%3F,(known%20as%20independent%20variables)."
  },
  {
    "objectID": "posts/1-probability/index.html",
    "href": "posts/1-probability/index.html",
    "title": "Probability Theory & Random Variables",
    "section": "",
    "text": "This blog will be all about probability theory & random variables…. maybe histograms?\nWe could write about predicting probabiliyt an dhow it has to add to 1?\nWe will be using the Diabetes dataset from sklearn. This dataset contains 442 total samples with 10 different features: * age * sex * body mass index (BMI) * average blood pressure * total serum chloesterol * low-density lipoproteins * total cholesterol * possibly log of serum triglycerides level * blood sugar level\nThis dataset is publically avaliable through the sklearn package with [documentation avaliable].(https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sarah Ramboyong",
    "section": "",
    "text": "Hello and welcome to my blog!\nSarah is a Masters of Engineering student at Virginia Tech studying Computer Science, with a concentration in Data Analytics and AI. When not studying, Sarah enjoys hiking, traveling, and spending time with her friends and family.\n\n\nVirginia Tech | Blacksburg, VA MEng in Computer Science | Aug 2022 - May 2024\nVirginia Tech | Blacksburg, VA B.s in Computational Modeling & Data Analytics | Aug 2020 - May 2023\n\n\n\nThis blog was created as an assignment for Virginia Tech’s CS 5805: Machine Learning. Sarah would like to express her gratitude to the teaching staff of this course, and especially to Dr. Martin Skarzynski for a great semester."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Sarah Ramboyong",
    "section": "",
    "text": "Virginia Tech | Blacksburg, VA MEng in Computer Science | Aug 2022 - May 2024\nVirginia Tech | Blacksburg, VA B.s in Computational Modeling & Data Analytics | Aug 2020 - May 2023"
  },
  {
    "objectID": "about.html#acknowledgement",
    "href": "about.html#acknowledgement",
    "title": "Sarah Ramboyong",
    "section": "",
    "text": "This blog was created as an assignment for Virginia Tech’s CS 5805: Machine Learning. Sarah would like to express her gratitude to the teaching staff of this course, and especially to Dr. Martin Skarzynski for a great semester."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS5805: Machine Learning Blog",
    "section": "",
    "text": "Probability Theory & Random Variables\n\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nSarah Ramboyong\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nSarah Ramboyong\n\n\n\n\n\n\n  \n\n\n\n\nLinear & Non-linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nSarah Ramboyong\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nSarah Ramboyong\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly & Outlier Detection\n\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nSarah Ramboyong\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2-clustering/index.html",
    "href": "posts/2-clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering a is an unsupervised machine learning technique that groups data points into a specified number of groups. This grouping is based on shared characteristics, so points in the same cluster are similar to one another and dissimilar to points in other clusters.\nThere are many appliciations of clustering, for example:\n\nBiology - identifying shared characteristics across different species\nMarketing - understanding consumer behavior\nCybersecurity - visualizing network traffic and patterns for improved anomaly detection\nImage segmentation - object recognition and isolation\n\nIn this blog post we will be implementing the K-Means clustering algorithm. We will be using the palmer penguin dataset which includes size measurements, clutch observations, and blood isotope ratios for 344 adult penguins observed on islands surrounding Palmer Station, Antarctica. This dataset is publically avaliable on GitHub.\n\n# Load the Palmer Penguins dataset\nimport pandas as pd\nfrom palmerpenguins import load_penguins\npenguins = load_penguins()\npenguins.dropna(inplace=True)  # drop nan values\nX = penguins.iloc[:, 2:6]\ny = pd.factorize(penguins[\"species\"])[0]"
  },
  {
    "objectID": "posts/2-clustering/index.html#references",
    "href": "posts/2-clustering/index.html#references",
    "title": "Clustering",
    "section": "References:",
    "text": "References:\n\nhttps://www.geeksforgeeks.org/clustering-in-machine-learning/\nhttps://www.datacamp.com/blog/clustering-in-machine-learning-5-essential-clustering-algorithms\nhttps://github.com/mcnakhaee/palmerpenguins\nhttps://www.w3schools.com/python/python_ml_k-means.asp\nhttps://builtin.com/data-science/elbow-method"
  },
  {
    "objectID": "posts/4-classification/index.html",
    "href": "posts/4-classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "Classification is a supervised machine learning technique that attempts to predict the correct label for the given data points. There are three phases in supervised learning.\n\nThe model is trained on a subset of the data and is given both the input data and its labels.\nIt is then evaluated on the testing subset.\nFinally, it predicts labels on new unseen data. This is called validation.\n\nMachine learning classification algorithms are split into two types: eager learners and lazy learners. Eager learners build the model from the training dataset before making any predictions on future datasets. Lazy learners memorize the training data and refer to those past inputs and outputs to make predictions on future points. For these reasons, eager learners are typically perform faster than lazy learners.\nIn this blog poast we will be implementing two different classification models, decision trees and K-Nearest Neighbor. Decision trees are eager learners while K-Nearest Neighbor is a lazy learning. We will be using the SMS Spam Collection dataset from the UCI Machine Learning Repository. This dataset contains 5,574 SMS messages in English, labeled ham (legitimate) or spam. This dataset is publically avaliable on The UCI Machine Learning Repository Website.\n\n# Load the SMS Spam Collection dataset\nimport pandas as pd\n\n# Load the SMS Spam Collection dataset\ndf = pd.read_csv(\"C:/Users/sarah/git/CS5805-Blog/posts/4-classification/SMSSpamCollection\", sep=\"\\t\", names=[\"label\", \"message\"])\ndf.head()\n\n\n\n\n\n\n\n\nlabel\nmessage\n\n\n\n\n0\nham\nGo until jurong point, crazy.. Available only ...\n\n\n1\nham\nOk lar... Joking wif u oni...\n\n\n2\nspam\nFree entry in 2 a wkly comp to win FA Cup fina...\n\n\n3\nham\nU dun say so early hor... U c already then say...\n\n\n4\nham\nNah I don't think he goes to usf, he lives aro...\n\n\n\n\n\n\n\nBefore we can perform classification algorithms we will first need todo some preprocessing steps. 1. Convert all text to lowercase to ensure that words are treated the same, regardless of case 2. Remove all punctuation to reduce noise 3. Remove all stop words. Stopwords are common words like ‘is’, ‘the’, ‘and’, etc. that usually don’t carry much information.\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport re\n\n# Download the stopwords from NLTK\nnltk.download('stopwords')\n\nstemmer = PorterStemmer()\n\n# Function to preprocess text\ndef preprocess(text):\n    # Convert to lower case\n    text = text.lower()\n    # Remove punctuation\n    text = re.sub('[^a-z]', ' ', text)\n    # Remove stopwords and stem the words\n    text = ' '.join(word for word in text.split() if word not in set(stopwords.words('english')))\n    return text\n\n# Apply the preprocessing to each message\ndf['message'] = df['message'].apply(preprocess)\ndf['message'].head()\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\sarah\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n0    go jurong point crazy available bugis n great ...\n1                              ok lar joking wif u oni\n2    free entry wkly comp win fa cup final tkts st ...\n3                  u dun say early hor u c already say\n4               nah think goes usf lives around though\nName: message, dtype: object\n\n\nNow we can begin performing classification."
  },
  {
    "objectID": "posts/4-classification/index.html#references",
    "href": "posts/4-classification/index.html#references",
    "title": "Classification",
    "section": "References:",
    "text": "References:\n\nhttps://www.datacamp.com/blog/classification-machine-learning\nhttps://www.datacamp.com/tutorial/decision-tree-classification-python\nhttps://archive.ics.uci.edu/dataset/228/sms+spam+collection\nhttps://www.labelf.ai/blog/what-is-accuracy-precision-recall-and-f1-score"
  }
]