{
  "hash": "e3ddedfcb4c736325eac46db2ff79365",
  "result": {
    "markdown": "---\ntitle: \"Clustering\"\nauthor: \"Sarah Ramboyong\"\ndate: \"2023-12-6\"\nimage: \"clustering_image.png\"\n---\n\n# Introduction \n\nClustering a is an unsupervised machine learning technique that groups data points into a specified number of groups. This grouping is based on shared characteristics, so points in the same cluster are similar to one another and dissimilar to points in other clusters.\n\nThere are many appliciations of clustering, for example:\n\n* Biology - identifying shared characteristics across different species\n* Marketing - understanding consumer behavior\n* Cybersecurity - visualizing network traffic and patterns for improved anomaly detection\n* Image segmentation - object recognition and isolation\n\nIn this blog post we will be implementing the K-Means clustering algorithm. We will be using the palmer penguin dataset which includes size measurements, clutch observations, and blood isotope ratios for 344 adult penguins observed on islands surrounding Palmer Station, Antarctica. This dataset is publically avaliable on [GitHub](https://github.com/mcnakhaee/palmerpenguins).\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Load the Palmer Penguins dataset\nimport pandas as pd\nfrom palmerpenguins import load_penguins\npenguins = load_penguins()\npenguins.dropna(inplace=True)  # drop nan values\nX = penguins.iloc[:, 2:6]\ny = pd.factorize(penguins[\"species\"])[0]\n```\n:::\n\n\n# K-Means\n\nK-Means is a clustering algorithm that groups data points into K clusters by minimzing the variance in each cluster.\n\nFirst we have to select the K number of clusters to group the data into. The elbow method is a popular method for selecting K. This method graphs inertia and K is the point on the graph where it begins to decreased linearly. \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nimport warnings\nwarnings.filterwarnings('ignore')\n\ninertias = []\n\nfor i in range(1,8):\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(X)\n    inertias.append(kmeans.inertia_)\n\nplt.plot(range(1,8), inertias, marker='o')\nplt.title('Elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=589 height=449}\n:::\n:::\n\n\nAccording to this plot, we see that 2 is a good value for K. sklearn is equipped with a built-in method to perform KMeans, so we will implement that using K=2 that we determined above.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nkmeans = KMeans(n_clusters=2)\nkmeans.fit(X)\n\n# The cluster centers\ncenters = kmeans.cluster_centers_\n\n# The labels for each data point\nlabels = kmeans.labels_\n```\n:::\n\n\nFinally, we can visualize these clusters with matplotlib. This dataset uses 4 features to calculate K-Means so first we will have to use PCA to reduce the dimensionality of our data to two dimensions, so that we can visualize on a 2D plot.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\n\n# Reduce the data to two dimensions using PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\n# Plot the data\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis')\n\n# Calculate the cluster centers in the PCA reduced space and plot them\ncenters_pca = pca.transform(kmeans.cluster_centers_)\nplt.scatter(centers_pca[:, 0], centers_pca[:, 1], s=300, c='red')\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=577 height=411}\n:::\n:::\n\n\nThis plot shows the results of running K-Means on the palmer penguin dataset using 2 clusters. The data points in the same clusters are highly similar to other points in the group, and dissimilar to points outside of it. There is little overlap between the two clusters, which indicates that there is a clear distinction between species. \n\n\n\n## References:\n* https://www.geeksforgeeks.org/clustering-in-machine-learning/\n* https://www.datacamp.com/blog/clustering-in-machine-learning-5-essential-clustering-algorithms\n* https://github.com/mcnakhaee/palmerpenguins\n* https://www.w3schools.com/python/python_ml_k-means.asp\n* https://builtin.com/data-science/elbow-method\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}