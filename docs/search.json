[
  {
    "objectID": "posts/5-anomaly/index.html",
    "href": "posts/5-anomaly/index.html",
    "title": "Anomaly & Outlier Detection",
    "section": "",
    "text": "Anomaly detection is the process of identifying suspicious or unusual occurrences within the dataset. These points are also called outliers and the can effect the performance of machine learning algorithms and skew results. In this blog we will implement one method of anomaly detection, Density-Based Spatial Clustering of Applications with Noise (DBSCAN). We will be using the Iris dataset from sklearn. This dataset contains 150 samples from 3 classes, 50 instances each. It contains the length and width of both sepals and petals.\nThis dataset is publically avaliable through the sklearn package with [documentation avaliable].(https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)\n\nfrom sklearn.datasets import load_iris\n\n# Load the iris dataset\niris = load_iris()\n\n# The data, target, and feature names are stored in the data, target, and feature_names attributes, respectively\nX = iris.data\ny = iris.target\nfeature_names = iris.feature_names"
  },
  {
    "objectID": "posts/5-anomaly/index.html#references",
    "href": "posts/5-anomaly/index.html#references",
    "title": "Anomaly & Outlier Detection",
    "section": "References:",
    "text": "References:\n\nhttps://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\nhttps://pieriantraining.com/dbscan-for-outlier-detection-in-python-and-scikit-learn-machine-learning-in-python/\nhttps://www.geeksforgeeks.org/detect-and-remove-the-outliers-using-python/\nhttps://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html#:~:text=low%20point%20density.-,Density%2DBased%20Spatial%20Clustering%20of%20Applications%20with%20Noise%20(DBSCAN),is%20containing%20noise%20and%20outliers."
  },
  {
    "objectID": "posts/3-linear/index.html",
    "href": "posts/3-linear/index.html",
    "title": "Linear & Non-linear Regression",
    "section": "",
    "text": "Regression is a statitical method that allows on to predict the output of one variables based on the input of another. It is a way of quantifying the relationship between a dependent variable (Y) and one or more independent variables (X).\nThere are two types of regression: linear and non-linear. Linear regression uses one independent variable to predict the outcome of the dependent variable. It follows the equation: \\(y=mx+b\\) where m is the slope, or the rate of change, and b is the intercept.\nNon-linear regression is used when there is a relationship between the dependent variable and more than one indepent variables. There are multiple types of non-linear regression, such as * polynomial regression * exponential regression * logarithmic regression\nNon-linear regression accounts for the majority of real-life data, which are complex and often influenced by multiple variables.\nIn this blog post we will use matplotlib and sklearn to plot different types of regression. We will also explore the ability to perform prediction using regression. We will be using the Boston Housing dataset from the StatLib archive. This dataset contains 506 samples and has a known linear relationship between variables. It has 14 different features: * Per capita crime rate by town * Proportion of residential land zoned for lots over 25,000 sq.ft. * Proportion of non-retail business acres per town. * Charles River dummy variable (1 if tract bounds river; 0 otherwise) * Nitric oxides concentration (parts per 10 million) * Average number of rooms per dwelling * Proportion of owner-occupied units built prior to 1940 * Weighted distances to five Boston employment centres * Index of accessibility to radial highways * Full-value property-tax rate per $10,000 * Pupil-teacher ratio by town * Proportion of blacks by town * % lower status of the population * Median value of owner-occupied homes in $1000’s\n\nimport pandas as pd\n\n# Load the Boston housing dataset\nboston = pd.read_csv(\"C:/Users/sarah/git/CS5805-Blog/posts/3-linear/BostonHousing.csv\")\nboston.head()\n\n\n\n\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nb\nlstat\nmedv\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n396.90\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n396.90\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n392.83\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n394.63\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n396.90\n5.33\n36.2"
  },
  {
    "objectID": "posts/3-linear/index.html#referenes",
    "href": "posts/3-linear/index.html#referenes",
    "title": "Linear & Non-linear Regression",
    "section": "Referenes:",
    "text": "Referenes:\n\nhttps://www.investopedia.com/terms/r/regression.asp#:~:text=%25%2025%25%200%25-,What%20Is%20a%20Regression%3F,(known%20as%20independent%20variables).\nhttps://www.seldon.io/machine-learning-regression-explained\nhttps://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html\nhttps://lib.stat.cmu.edu/datasets/boston\nhttps://pieriantraining.com/nonlinear-regression-in-machine-learning-python/"
  },
  {
    "objectID": "posts/1-probability/index.html",
    "href": "posts/1-probability/index.html",
    "title": "Probability Theory & Random Variables",
    "section": "",
    "text": "Bayes’ theroem is a well-known statistical theory that determines the likelihood of an event occuring given prior events. It follows the formula: \nThis theorm was discovered by mathematician Thomas Bayes in 1763, and has since been applied to machine learning.\nIn this blog post we will implement Naive Bayes Classifier, whcih applies Bayes’ theroem to perform classification. Naive Bayes can be implemented for different types of probability distributions, especially the most common * Multinomial * Bernoulli * Gaussian For the scope of this post we will be focusing on the Gaussian distribution. Will attempt to predict the probability of each classification label, both with built-in methods and by hand.\nWe will be using the Wine dataset from sklearn. This dataset contains 178 samples from 3 different classes of wine. It has 14 features: * Class * Alcohol * Malicacid * Ash * Alcalinity of ash * Magnesium * Total phenols * Flavanoids * Nonflavanoid phenols * Proanthocyanins * Color intensity * Hue * 0D280_0D315_of_diluted_wines * Proline\nThis dataset is publically avaliable through the sklearn package with documentation avaliable.\n\nfrom sklearn.datasets import load_wine\n\n# Load the Wine dataset\nwine_data = load_wine()\n\n# Get the data and labels\nX = wine_data.data\ny = wine_data.target"
  },
  {
    "objectID": "posts/1-probability/index.html#references",
    "href": "posts/1-probability/index.html#references",
    "title": "Probability Theory & Random Variables",
    "section": "References",
    "text": "References\n\nhttps://www.britannica.com/topic/Bayess-theorem#:~:text=The%20theorem%20was%20discovered%20among,of%20a%20parameter%20under%20investigation.\nhttps://www.datacamp.com/tutorial/naive-bayes-scikit-learn\nhttps://www.ibm.com/topics/naive-bayes#:~:text=As%20a%20result%2C%20it’s%20one,the%20conditional%20independence%20assumption%20holds."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sarah Ramboyong",
    "section": "",
    "text": "Hello and welcome to my blog!\nSarah is a Masters of Engineering student at Virginia Tech studying Computer Science, with a concentration in Data Analytics and AI. When not studying, Sarah enjoys hiking, traveling, and spending time with her friends and family.\n\n\nVirginia Tech | Blacksburg, VA MEng in Computer Science | Aug 2022 - May 2024\nVirginia Tech | Blacksburg, VA B.s in Computational Modeling & Data Analytics | Aug 2020 - May 2023\n\n\n\nThis blog was created as an assignment for Virginia Tech’s CS 5805: Machine Learning. Sarah would like to express her gratitude to the teaching staff of this course, and especially to Dr. Martin Skarzynski for a great semester."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Sarah Ramboyong",
    "section": "",
    "text": "Virginia Tech | Blacksburg, VA MEng in Computer Science | Aug 2022 - May 2024\nVirginia Tech | Blacksburg, VA B.s in Computational Modeling & Data Analytics | Aug 2020 - May 2023"
  },
  {
    "objectID": "about.html#acknowledgement",
    "href": "about.html#acknowledgement",
    "title": "Sarah Ramboyong",
    "section": "",
    "text": "This blog was created as an assignment for Virginia Tech’s CS 5805: Machine Learning. Sarah would like to express her gratitude to the teaching staff of this course, and especially to Dr. Martin Skarzynski for a great semester."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS5805: Machine Learning Blog",
    "section": "",
    "text": "Probability Theory & Random Variables\n\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nSarah Ramboyong\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nSarah Ramboyong\n\n\n\n\n\n\n  \n\n\n\n\nLinear & Non-linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nSarah Ramboyong\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nSarah Ramboyong\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly & Outlier Detection\n\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nSarah Ramboyong\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2-clustering/index.html",
    "href": "posts/2-clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering a is an unsupervised machine learning technique that groups data points into a specified number of groups. This grouping is based on shared characteristics, so points in the same cluster are similar to one another and dissimilar to points in other clusters.\nThere are many appliciations of clustering, for example:\n\nBiology - identifying shared characteristics across different species\nMarketing - understanding consumer behavior\nCybersecurity - visualizing network traffic and patterns for improved anomaly detection\nImage segmentation - object recognition and isolation\n\nIn this blog post we will be implementing the K-Means clustering algorithm. We will be using the palmer penguin dataset which includes size measurements, clutch observations, and blood isotope ratios for 344 adult penguins observed on islands surrounding Palmer Station, Antarctica. This dataset is publically avaliable on GitHub.\n\n# Load the Palmer Penguins dataset\nimport pandas as pd\nfrom palmerpenguins import load_penguins\npenguins = load_penguins()\npenguins.dropna(inplace=True)  # drop nan values\nX = penguins.iloc[:, 2:6]\ny = pd.factorize(penguins[\"species\"])[0]"
  },
  {
    "objectID": "posts/2-clustering/index.html#references",
    "href": "posts/2-clustering/index.html#references",
    "title": "Clustering",
    "section": "References:",
    "text": "References:\n\nhttps://www.geeksforgeeks.org/clustering-in-machine-learning/\nhttps://www.datacamp.com/blog/clustering-in-machine-learning-5-essential-clustering-algorithms\nhttps://github.com/mcnakhaee/palmerpenguins\nhttps://www.w3schools.com/python/python_ml_k-means.asp\nhttps://builtin.com/data-science/elbow-method"
  },
  {
    "objectID": "posts/4-classification/index.html",
    "href": "posts/4-classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "Classification is a supervised machine learning technique that attempts to predict the correct label for the given data points. There are three phases in supervised learning.\n\nThe model is trained on a subset of the data and is given both the input data and its labels.\nIt is then evaluated on the testing subset.\nFinally, it predicts labels on new unseen data. This is called validation.\n\nMachine learning classification algorithms are split into two types: eager learners and lazy learners. Eager learners build the model from the training dataset before making any predictions on future datasets. Lazy learners memorize the training data and refer to those past inputs and outputs to make predictions on future points. For these reasons, eager learners are typically perform faster than lazy learners.\nIn this blog post we will be implementing two different classification models, decision trees and K-Nearest Neighbor. Decision trees are eager learners while K-Nearest Neighbor is a lazy learning. We will be using the SMS Spam Collection dataset from the UCI Machine Learning Repository. This dataset contains 5,574 SMS messages in English, labeled ham (legitimate) or spam. This dataset is publically avaliable on The UCI Machine Learning Repository Website.\n\n# Load the SMS Spam Collection dataset\nimport pandas as pd\n\n# Load the SMS Spam Collection dataset\ndf = pd.read_csv(\"C:/Users/sarah/git/CS5805-Blog/posts/4-classification/SMSSpamCollection\", sep=\"\\t\", names=[\"label\", \"message\"])\ndf.head()\n\n\n\n\n\n\n\n\nlabel\nmessage\n\n\n\n\n0\nham\nGo until jurong point, crazy.. Available only ...\n\n\n1\nham\nOk lar... Joking wif u oni...\n\n\n2\nspam\nFree entry in 2 a wkly comp to win FA Cup fina...\n\n\n3\nham\nU dun say so early hor... U c already then say...\n\n\n4\nham\nNah I don't think he goes to usf, he lives aro...\n\n\n\n\n\n\n\nBefore we can perform classification algorithms we will first need todo some preprocessing steps. 1. Convert all text to lowercase to ensure that words are treated the same, regardless of case 2. Remove all punctuation to reduce noise 3. Remove all stop words. Stopwords are common words like ‘is’, ‘the’, ‘and’, etc. that usually don’t carry much information.\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport re\n\n# Download the stopwords from NLTK\nnltk.download('stopwords')\n\nstemmer = PorterStemmer()\n\n# Function to preprocess text\ndef preprocess(text):\n    # Convert to lower case\n    text = text.lower()\n    # Remove punctuation\n    text = re.sub('[^a-z]', ' ', text)\n    # Remove stopwords and stem the words\n    text = ' '.join(word for word in text.split() if word not in set(stopwords.words('english')))\n    return text\n\n# Apply the preprocessing to each message\ndf['message'] = df['message'].apply(preprocess)\ndf['message'].head()\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\sarah\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n0    go jurong point crazy available bugis n great ...\n1                              ok lar joking wif u oni\n2    free entry wkly comp win fa cup final tkts st ...\n3                  u dun say early hor u c already say\n4               nah think goes usf lives around though\nName: message, dtype: object\n\n\nNow we can begin performing classification."
  },
  {
    "objectID": "posts/4-classification/index.html#references",
    "href": "posts/4-classification/index.html#references",
    "title": "Classification",
    "section": "References:",
    "text": "References:\n\nhttps://www.datacamp.com/blog/classification-machine-learning\nhttps://www.datacamp.com/tutorial/decision-tree-classification-python\nhttps://archive.ics.uci.edu/dataset/228/sms+spam+collection\nhttps://www.labelf.ai/blog/what-is-accuracy-precision-recall-and-f1-score"
  }
]