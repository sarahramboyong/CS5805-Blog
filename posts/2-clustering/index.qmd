---
title: "Clustering"
author: "Sarah Ramboyong"
date: "2023-12-6"
image: "clustering_image.png"
jupyter: python3
---
# Introduction 

Clustering a is an unsupervised machine learning technique that groups data points into a specified number of groups. This grouping is based on shared characteristics, so points in the same cluster are similar to one another and dissimilar to points in other clusters.

There are many appliciations of clustering, for example:

* Biology - identifying shared characteristics across different species
* Marketing - understanding consumer behavior
* Cybersecurity - visualizing network traffic and patterns for improved anomaly detection
* Image segmentation - object recognition and isolation

In this blog post we will be implementing the K-Means clustering algorithm. We will be using the palmer penguin dataset which includes size measurements, clutch observations, and blood isotope ratios for 344 adult penguins observed on islands surrounding Palmer Station, Antarctica. This dataset is publically avaliable on [GitHub](https://github.com/mcnakhaee/palmerpenguins).

```{python}
# Load the Palmer Penguins dataset
import pandas as pd
from palmerpenguins import load_penguins
penguins = load_penguins()
penguins.dropna(inplace=True)  # drop nan values
X = penguins.iloc[:, 2:6]
y = pd.factorize(penguins["species"])[0]
```


# K-Means

K-Means is a clustering algorithm that groups data points into K clusters by minimzing the variance in each cluster.

First we have to select the K number of clusters to group the data into. The elbow method is a popular method for selecting K. This method graphs inertia and K is the point on the graph where it begins to decreased linearly. 

```{python}
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import warnings
warnings.filterwarnings('ignore')

inertias = []

for i in range(1,8):
    kmeans = KMeans(n_clusters=i)
    kmeans.fit(X)
    inertias.append(kmeans.inertia_)

plt.plot(range(1,8), inertias, marker='o')
plt.title('Elbow method')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.show()
```

According to this plot, we see that 2 is a good value for K. sklearn is equipped with a built-in method to perform KMeans, so we will implement that using K=2 that we determined above.

```{python}
kmeans = KMeans(n_clusters=2)
kmeans.fit(X)

# The cluster centers
centers = kmeans.cluster_centers_

# The labels for each data point
labels = kmeans.labels_
```

Finally, we can visualize these clusters with matplotlib. This dataset uses 4 features to calculate K-Means so first we will have to use PCA to reduce the dimensionality of our data to two dimensions, so that we can visualize on a 2D plot.

```{python}
from sklearn.decomposition import PCA

# Reduce the data to two dimensions using PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# Plot the data
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis')

# Calculate the cluster centers in the PCA reduced space and plot them
centers_pca = pca.transform(kmeans.cluster_centers_)
plt.scatter(centers_pca[:, 0], centers_pca[:, 1], s=300, c='red')

plt.show()
```

This plot shows the results of running K-Means on the palmer penguin dataset using 2 clusters. The data points in the same clusters are highly similar to other points in the group, and dissimilar to points outside of it. There is little overlap between the two clusters, which indicates that there is a clear distinction between species. 



## References:
* https://www.geeksforgeeks.org/clustering-in-machine-learning/
* https://www.datacamp.com/blog/clustering-in-machine-learning-5-essential-clustering-algorithms
* https://github.com/mcnakhaee/palmerpenguins
* https://www.w3schools.com/python/python_ml_k-means.asp
* https://builtin.com/data-science/elbow-method
