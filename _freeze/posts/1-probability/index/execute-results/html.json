{
  "hash": "0c3a26a1baf2542b612b8e28282090b1",
  "result": {
    "markdown": "---\ntitle: \"Probability Theory & Random Variables\"\nauthor: \"Sarah Ramboyong\"\ndate: \"2023-12-6\"\nimage: \"probability_image.png\"\n---\n\n# Introduction\n\nBayes' theroem is a well-known statistical theory that determines the likelihood of an event occuring given prior events. It follows the formula:\n![](bayes_thm.jpg)\n\nThis theorm was discovered by mathematician Thomas Bayes in 1763, and has since been applied to machine learning. \n\nIn this blog post we will implement Naive Bayes Classifier, whcih applies Bayes' theroem to perform classification. Naive Bayes can be implemented for different types of probability distributions, especially the most common \n* Multinomial\n* Bernoulli\n* Gaussian\nFor the scope of this post we will be focusing on the Gaussian distribution. Will attempt to predict the probability of each classification label, both with built-in methods and by hand.\n\n\nWe will be using the Wine dataset from sklearn. This dataset contains 178 samples from 3 different classes of wine. It has 14 features:\n* Class \n* Alcohol\n* Malicacid\n* Ash\n* Alcalinity of ash\n* Magnesium\n* Total phenols\n* Flavanoids\n* Nonflavanoid phenols\n* Proanthocyanins\n* Color intensity\n* Hue\n* 0D280_0D315_of_diluted_wines\n* Proline\n\nThis dataset is publically avaliable through the sklearn package with [documentation avaliable](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html).\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.datasets import load_wine\n\n# Load the Wine dataset\nwine_data = load_wine()\n\n# Get the data and labels\nX = wine_data.data\ny = wine_data.target\n```\n:::\n\n\n# Guassian Naive Bayes\n\nNaive Bayes is a supervised machine learning algorithm based on Bayes' theorm. It is used to perfrom classification. This algorithm assumes that features contribute equally and independently to the outcome.\n\nSklearn has a built-in method to perform Guassian Naive Bayes. We can build and train the model then predict the probabilities on the test data.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\n# Split the dataset into a training set and a testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a GaussianNB object\ngnb = GaussianNB()\n\n# Fit the model to the training data\ngnb.fit(X_train, y_train)\n\n# Predict probabilities on the test data\ny_pred_proba = gnb.predict_proba(X_test)\n\nprint(y_pred_proba)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[9.99991820e-01 8.17986629e-06 1.75844851e-31]\n [9.99999053e-01 9.46598997e-07 1.78086968e-25]\n [1.13448660e-18 1.99953560e-03 9.98000464e-01]\n [9.99999999e-01 7.71694277e-10 4.13905530e-37]\n [4.85016095e-07 9.99999515e-01 1.99877483e-22]\n [1.00000000e+00 2.47066704e-12 1.42452382e-33]\n [7.95594246e-11 1.00000000e+00 6.55702700e-13]\n [3.15918793e-21 4.37517039e-12 1.00000000e+00]\n [1.94710799e-04 9.99805289e-01 5.67647705e-29]\n [2.42169478e-15 5.33549614e-04 9.99466450e-01]\n [9.93827013e-01 6.17298732e-03 1.83781764e-32]\n [1.37658180e-18 3.84803880e-12 1.00000000e+00]\n [9.86542823e-01 1.34571766e-02 3.14047577e-20]\n [7.68155801e-16 9.82464750e-01 1.75352499e-02]\n [1.00000000e+00 2.78016158e-13 4.69430467e-35]\n [1.25924089e-07 9.99999874e-01 2.81153369e-15]\n [4.42564516e-11 1.00000000e+00 4.05508262e-12]\n [2.99568596e-12 1.00000000e+00 1.13161503e-13]\n [1.00000000e+00 1.16206194e-11 9.87868150e-40]\n [1.13133637e-08 9.99999989e-01 2.19397345e-19]\n [1.00000000e+00 4.04187890e-22 6.17606318e-59]\n [3.18591996e-01 6.81408004e-01 6.80506976e-35]\n [2.43168670e-16 9.99826495e-01 1.73505362e-04]\n [1.23408452e-19 2.98728031e-13 1.00000000e+00]\n [7.10903161e-29 2.15985533e-18 1.00000000e+00]\n [7.07192544e-24 1.17724326e-18 1.00000000e+00]\n [4.53176013e-08 9.99999955e-01 2.35118322e-16]\n [1.36668645e-02 9.86333136e-01 1.76260378e-16]\n [8.04757256e-14 9.99999270e-01 7.30054928e-07]\n [9.99999997e-01 2.70647612e-09 9.92082030e-30]\n [9.99999305e-01 6.94947125e-07 9.14210749e-31]\n [6.43006687e-09 9.99999994e-01 1.37380635e-16]\n [8.16729277e-22 7.29635235e-06 9.99992704e-01]\n [1.00000000e+00 3.47818916e-15 1.21420599e-41]\n [1.00000000e+00 1.88697780e-10 2.34380035e-32]\n [9.99999997e-01 2.75077349e-09 2.65193217e-46]]\n```\n:::\n:::\n\n\nThis 2D array shows the probability of each test point being classified for each class. From basic probability and Bayes' theorem we know that the probabilites of each sub-array should add up to 1. We can perform a simple check. \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\n\n# Calculate the sum of each sub-array\nsums = np.sum(y_pred_proba, axis=1)\n\nprint(np.round(sums, 1) == 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[ True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True]\n```\n:::\n:::\n\n\nWe round to take care of any repeating decimals. For example, the first extry in the array before rounding was 1.000000000000001. The check above shows that the probabilities of each sub-array do in fact sum to 1. \n\n# Predicting Classification Probability By Hand\n\nHaving a built-in fuction provided to us is very convient, however, it doesn't allow us to fully appreciate the application of Bayes' Theorem. We will calculate these classificaiton probabilities by hand and compare our calculations to the results of sklearn's built-in method. \n\nFirst we will calculate the prior probabilities, or the proportion of instances of that class in the dataset.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\n\n# Calculate the prior probabilities\nprior_probs = np.bincount(y_train) / len(y_train)\n```\n:::\n\n\nNow we will calcualte the likelihood. The likelihood is the probability of the data given the class, and we will use the Gaussian normal distribution formula:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom scipy.stats import norm\n\n# Calculate the mean and standard deviation for each feature for each class\nmeans = np.array([X_train[y_train == i].mean(axis=0) for i in np.unique(y_train)])\nstds = np.array([X_train[y_train == i].std(axis=0) for i in np.unique(y_train)])\n\n# Calculate the likelihood\nlikelihoods = [norm(loc=means[i], scale=stds[i]).pdf(X_test) for i in np.unique(y_train)]\n```\n:::\n\n\nNext we will apply Bayes' Theorem using the calulations performed in the previous two steps.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Apply Bayes' theorem\nposterior_probs = [prior_probs[i] * np.prod(likelihoods[i], axis=1) for i in np.unique(y_train)]\n```\n:::\n\n\nFinally, we will normalize these probabilites to sum to 1.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Normalize the posterior probabilities\nposterior_probs = np.array(posterior_probs).T\nposterior_probs = posterior_probs / posterior_probs.sum(axis=1, keepdims=True)\n\nprint(posterior_probs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[9.99991700e-01 8.29987299e-06 1.60938552e-31]\n [9.99999054e-01 9.45532479e-07 1.65168801e-25]\n [1.00757757e-18 1.99830633e-03 9.98001694e-01]\n [9.99999999e-01 7.66094807e-10 3.63189055e-37]\n [4.84902416e-07 9.99999515e-01 1.78295768e-22]\n [1.00000000e+00 2.44562403e-12 1.28719895e-33]\n [7.84509114e-11 1.00000000e+00 6.21497679e-13]\n [2.90891765e-21 4.32284791e-12 1.00000000e+00]\n [1.95571203e-04 9.99804429e-01 5.15534018e-29]\n [2.30552490e-15 5.35092786e-04 9.99464907e-01]\n [9.93882534e-01 6.11746635e-03 1.58918556e-32]\n [1.28452945e-18 3.85660910e-12 1.00000000e+00]\n [9.86553423e-01 1.34465768e-02 2.92576879e-20]\n [7.15994008e-16 9.82564613e-01 1.74353866e-02]\n [1.00000000e+00 2.77942746e-13 4.10380310e-35]\n [1.25569301e-07 9.99999874e-01 2.67261512e-15]\n [4.32542473e-11 1.00000000e+00 3.85026190e-12]\n [2.90319319e-12 1.00000000e+00 1.02367466e-13]\n [1.00000000e+00 1.15793759e-11 8.70345678e-40]\n [1.10128578e-08 9.99999989e-01 2.06469733e-19]\n [1.00000000e+00 4.02312068e-22 5.24692161e-59]\n [3.18946528e-01 6.81053472e-01 5.85549525e-35]\n [2.00667093e-16 9.99836216e-01 1.63783730e-04]\n [1.18261321e-19 2.99427793e-13 1.00000000e+00]\n [6.16841691e-29 2.13123817e-18 1.00000000e+00]\n [6.55609746e-24 1.16259746e-18 1.00000000e+00]\n [4.53638332e-08 9.99999955e-01 2.19344400e-16]\n [1.35808559e-02 9.86419144e-01 1.61734314e-16]\n [7.41663507e-14 9.99999295e-01 7.04700897e-07]\n [9.99999997e-01 2.71500052e-09 9.15171905e-30]\n [9.99999311e-01 6.89145891e-07 8.16290642e-31]\n [6.30992893e-09 9.99999994e-01 1.31216900e-16]\n [7.36198919e-22 7.25068392e-06 9.99992749e-01]\n [1.00000000e+00 3.46489065e-15 1.08835365e-41]\n [1.00000000e+00 1.86862919e-10 2.10177338e-32]\n [9.99999997e-01 2.80751532e-09 2.38103607e-46]]\n```\n:::\n:::\n\n\nNow we can check our work by comparing these calculations to the output of sklearn's built-in function.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nprint(np.round(y_pred_proba, 2) == np.round(posterior_probs, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]\n [ True  True  True]]\n```\n:::\n:::\n\n\nWe see that our calculations match and we have successfully calculated the classificaiton probabilities. \n\n# Probability Histogram\n\nWe can visualize these predicted probabilities by plotting the probability histograms. \n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\n# Predict probabilities on the test data\ny_pred_proba = gnb.predict_proba(X_test)\n\n# Plot a histogram for each class\nfor i in range(y_pred_proba.shape[1]):\n    plt.hist(y_pred_proba[:, i], bins=10, label=str(i), alpha=0.7)\n\nplt.xlabel('Predicted Probability')\nplt.ylabel('Frequency')\nplt.title('Probability Histograms')\nplt.legend(title='Class')\nplt.xticks(np.arange(0, 1.1, 0.1))\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=585 height=449}\n:::\n:::\n\n\nFrom this we see that Class 2 is either predicted to have a very low probability of classification or a very high probability. Class 1 is predicted to have a high probability of classiication, while Class 0 appears to be infrequent but with a high probability.\n\n# Prediction and Evaluation\n\nAfter performing these calulations, the Naive Bayes algorithm selects the highest posterior probability to determine which label to classify the data point as. We will now finish predicting the labels of the test data set and evaluate the model.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.metrics import accuracy_score\n\n# Predict the labels of the test data\ny_pred = gnb.predict(X_test)\n\n# Calculate the accuracy of the model\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(\"Accuracy: \", accuracy)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy:  1.0\n```\n:::\n:::\n\n\nOverall, Naive Bayes is generally considered a good algorithm that performs both fast and fairly accurate. We see that our model achieved an accuracy of 100%, and this is largely due to the features adhearing to the conditional independence assumption. \n\n## References\n* https://www.britannica.com/topic/Bayess-theorem#:~:text=The%20theorem%20was%20discovered%20among,of%20a%20parameter%20under%20investigation.\n* https://www.datacamp.com/tutorial/naive-bayes-scikit-learn\n* https://www.ibm.com/topics/naive-bayes#:~:text=As%20a%20result%2C%20it's%20one,the%20conditional%20independence%20assumption%20holds.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}